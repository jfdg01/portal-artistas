### robots.txt

# Allow major search engines
User-agent: Googlebot
Disallow:

User-agent: AdsBot-Google
Disallow:

User-agent: Google-InspectionTool
Disallow:

User-agent: Bingbot
Disallow:

User-agent: DuckDuckBot
Disallow:

User-agent: Slurp
Disallow:

User-agent: Applebot
Disallow:

# Block known aggressive scrapers/crawlers
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: YandexBot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: Sogou spider
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: DataForSeoBot
Disallow: /

User-agent: GPTBot
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: anthropic-ai
Disallow: /

# Block common programmatic clients often used for scraping
User-agent: python-requests
Disallow: /

User-agent: curl
Disallow: /

User-agent: wget
Disallow: /

User-agent: Go-http-client
Disallow: /

User-agent: aiohttp
Disallow: /

User-agent: Scrapy
Disallow: /

User-agent: httpx
Disallow: /

# Default: disallow all other bots
User-agent: *
Disallow: /
